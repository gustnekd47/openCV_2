{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install opencv-python\n",
    "# ! pip install tensorflow==1.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "# from tensorflow.python.framework import graph_util\n",
    "# from tensorflow.python.platform import gfile\n",
    "\n",
    "# tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# mnist = input_data.read_data_sets('./MNIST_data/', one_hot=True)\n",
    "\n",
    "# #\n",
    "# # hyper parameters\n",
    "# #\n",
    "# learning_rate = 0.001\n",
    "# training_epochs = 20\n",
    "# batch_size = 100\n",
    "\n",
    "# #\n",
    "# # Model configuration\n",
    "# #\n",
    "# X = tf.placeholder(tf.float32, [None, 28, 28, 1], name='data')\n",
    "# Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# conv1 = tf.layers.conv2d(X, 10, [3, 3], padding='same', activation=tf.nn.relu)\n",
    "# pool1 = tf.layers.max_pooling2d(conv1, [2, 2], strides=2, padding='same')\n",
    "\n",
    "# conv2 = tf.layers.conv2d(pool1, 20, [3, 3], padding='same', activation=tf.nn.relu)\n",
    "# pool2 = tf.layers.max_pooling2d(conv2, [2, 2], strides=2, padding='same')\n",
    "\n",
    "# fc1 = tf.contrib.layers.flatten(pool2)\n",
    "# fc2 = tf.layers.dense(fc1, 200, activation=tf.nn.relu)\n",
    "# logits = tf.layers.dense(fc2, 10, activation=None)\n",
    "# output = tf.nn.softmax(logits, name='prob')\n",
    "\n",
    "# cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
    "# optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# #\n",
    "# # Training\n",
    "# #\n",
    "# sess = tf.Session()\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "# total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "# print('Start learning!')\n",
    "# for epoch in range(training_epochs):\n",
    "#     total_cost = 0\n",
    "\n",
    "#     for i in range(total_batch):\n",
    "#         batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "#         batch_xs = batch_xs.reshape(-1, 28, 28, 1)\n",
    "#         _, cost_val = sess.run([optimizer, cost], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "#         total_cost += cost_val\n",
    "\n",
    "#     print('Epoch: {0}, Avg. Cost = {1:.4f}'.format(epoch + 1, total_cost/total_batch))\n",
    "\n",
    "# print('Learning finished!')\n",
    "\n",
    "# # Test the results\n",
    "# is_correct = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "# acc = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "# accuracy = sess.run(acc, feed_dict={\n",
    "#                     X: mnist.test.images.reshape(-1, 28, 28, 1), Y: mnist.test.labels})\n",
    "# print('Test Accuracy:', accuracy)\n",
    "\n",
    "# # Freeze variables and save pb file\n",
    "# output_graph_def = graph_util.convert_variables_to_constants(sess, sess.graph_def, ['prob'])\n",
    "# with gfile.FastGFile('./mnist_cnn.pb', 'wb') as f:\n",
    "#     f.write(output_graph_def.SerializeToString())\n",
    "\n",
    "# print('mnist_cnn.pb file is created successfully!!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning을 이용한 필기체 인식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv2.dnn.readNet(model, config=None, framework=None) -> retval\n",
    "    # model:훈련된 가중치 파일\n",
    "    # config: 네트워크 구성을 저장하고 있는 텍스트 파일 이름\n",
    "    # framework: None\n",
    "    # retval: cv2.dnn_Net 클래스 객체\n",
    "    \n",
    "# cv2.dnn.blobFromImage(image, scalefactor=None, size=None, mean=None,\n",
    "#                         swapRB=None, crop=None, ddepth=None)\n",
    "    # image: 입력영상\n",
    "    # scalefactor: 입력영상 픽셀에 곱할 값, 기본값은 1\n",
    "    # size: 츨력영상의 크기, 기본값은 (0,0)\n",
    "    # mean: 입력영상 각 채널에서 뺄 평균값\n",
    "    # swapRB: R,B 채널의 순서, True or False\n",
    "    # ddepth: 출력영상의 깊이 CV_32F\n",
    "    # retval:blob객체, numpy.ndarray, shape=(N,C,H,W), dtype=numpy.float32\n",
    "\n",
    "# net.setInput(blob, name=None, scalefactor=None, mean=None)-> None\n",
    "    # blob: blob객체\n",
    "    # name\n",
    "    # scalefactor\n",
    "    # mean\n",
    "\n",
    "# net.forward(outputName=None) -> retval\n",
    "    # outputName: 출력레이어 이름\n",
    "    # retval: dnn 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6428/34265761.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "oldx, oldy = -1, -1\n",
    "\n",
    "\n",
    "def on_mouse(event, x, y, flags, _):\n",
    "    global oldx, oldy\n",
    "\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        oldx, oldy = x, y\n",
    "\n",
    "    elif event == cv2.EVENT_LBUTTONUP:\n",
    "        oldx, oldy = -1, -1\n",
    "\n",
    "    elif event == cv2.EVENT_MOUSEMOVE:\n",
    "        if flags & cv2.EVENT_FLAG_LBUTTON:\n",
    "            cv2.line(img, (oldx, oldy), (x, y), (255, 255, 255), 40, cv2.LINE_AA)\n",
    "            oldx, oldy = x, y\n",
    "            cv2.imshow('img', img)\n",
    "\n",
    "\n",
    "net = cv2.dnn.readNet('mnist_cnn.pb')\n",
    "\n",
    "if net.empty():\n",
    "    print('Network load failed!')\n",
    "    sys.exit()\n",
    "\n",
    "img = np.zeros((400, 400), np.uint8)\n",
    "\n",
    "cv2.imshow('img', img)\n",
    "cv2.setMouseCallback('img', on_mouse)\n",
    "\n",
    "while True:\n",
    "    c = cv2.waitKey()\n",
    "\n",
    "    if c == 27:\n",
    "        break\n",
    "    elif c == ord(' '):\n",
    "        blob = cv2.dnn.blobFromImage(img, 1/255., (28, 28))\n",
    "        net.setInput(blob)\n",
    "        prob = net.forward()\n",
    "\n",
    "        _, maxVal, _, maxLoc = cv2.minMaxLoc(prob)\n",
    "        digit = maxLoc[0] # 판단된\n",
    "\n",
    "        print(f'{digit} ({maxVal * 100:4.2f}%)')\n",
    "\n",
    "        img.fill(0)\n",
    "        cv2.imshow('img', img)\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GoogleNet 영상인식 모델 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://modelzoo.co/model/googlenet-v2 모델파일 다운로드\n",
    "\n",
    "# Caffe Model Zoo : github.com/BVLC/caffe\n",
    "## 모델 파일 : dl.caffe.berkeleyvision.org/bvlc_googlenet.caffemodel\n",
    "## 설정 파일 : github.com/BVLC/caffe/blob/master/models/bvlc_googlenet/deploy.prototxt\n",
    "\n",
    "\n",
    "# ONNX Model Zoo : github.com/onnx/models\n",
    "\n",
    "# 클래스 이름 파일 : github.com/opencv/opencv/blob/4.1.0/samples/data/dnn/classification_classes_ILSVRC2012.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "# 입력 영상 불러오기\n",
    "\n",
    "# filename = 'apple1.png'\n",
    "filename = './googlenet/pineapple.jpg'\n",
    "\n",
    "# if len(sys.argv) > 1: \n",
    "#     filename = sys.argv[1]\n",
    "\n",
    "img = cv2.imread(filename)\n",
    "\n",
    "if img is None:\n",
    "    print('Image load failed!')\n",
    "    sys.exit()\n",
    "\n",
    "# 네트워크 불러오기\n",
    "\n",
    "# Caffe\n",
    "model = 'googlenet/bvlc_googlenet.caffemodel'\n",
    "config = 'googlenet/deploy.prototxt.txt'\n",
    "\n",
    "# ONNX\n",
    "# model = 'googlenet/googlenet-9.onnx'\n",
    "# config = ''\n",
    "\n",
    "net = cv2.dnn.readNet(model, config)\n",
    "\n",
    "if net.empty():\n",
    "    print('Network load failed!')\n",
    "    sys.exit()\n",
    "\n",
    "# 클래스 이름 불러오기\n",
    "\n",
    "classNames = []\n",
    "with open('googlenet/classification_classes_ILSVRC2012.txt', 'rt') as f:\n",
    "    classNames = f.read().rstrip('\\n').split('\\n')\n",
    "\n",
    "# 추론\n",
    "\n",
    "blob = cv2.dnn.blobFromImage(img, 1, (224, 224), (104, 117, 123))\n",
    "net.setInput(blob)\n",
    "prob = net.forward()\n",
    "\n",
    "# 추론 결과 확인 & 화면 출력\n",
    "\n",
    "out = prob.flatten()\n",
    "classId = np.argmax(out)\n",
    "confidence = out[classId]\n",
    "\n",
    "text = f'{classNames[classId]} ({confidence * 100:4.2f}%)'\n",
    "cv2.putText(img, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "cv2.imshow('img', img)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenCV DNN 얼굴검출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/opencv/opencv/tree/master/samples/dnn/face_detector\n",
    "# deploy.prototxt.txt, download-weights.py.txt, opencv_face_detector.pbtxt.text 다운로드\n",
    "\n",
    "# Caffe    https://raw.githubusercontent.com/opencv/opencv_3rdparty/dnn_samples_face_detector_20180205_fp16/res10_300x300_ssd_iter_140000_fp16.caffemodel\n",
    "\n",
    "# Tensorflow  https://raw.githubusercontent.com/opencv/opencv_3rdparty/dnn_samples_face_detector_20180220_uint8/opencv_face_detector_uint8.pb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "# model = 'opencv_face_detector/res10_300x300_ssd_iter_140000_fp16.caffemodel'\n",
    "# config = 'opencv_face_detector/deploy.prototxt'\n",
    "model = 'opencv_face_detector/opencv_face_detector_uint8.pb'\n",
    "config = 'opencv_face_detector/opencv_face_detector.pbtxt'\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print('Camera open failed!')\n",
    "    sys.exit()\n",
    "\n",
    "net = cv2.dnn.readNet(model, config)\n",
    "\n",
    "if net.empty():\n",
    "    print('Net open failed!')\n",
    "    sys.exit()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1, (300, 300), (104, 177, 123))\n",
    "    net.setInput(blob)\n",
    "    out = net.forward()# out.shape=(1,1, 200, 7)\n",
    "\n",
    "    detect = out[0, 0, :, :] ##0, 0, 사용안함\n",
    "    (h, w) = frame.shape[:2]\n",
    "\n",
    "    for i in range(detect.shape[0]):\n",
    "        confidence = detect[i, 2]\n",
    "        if confidence < 0.5:\n",
    "            break\n",
    "\n",
    "        x1 = int(detect[i, 3] * w)\n",
    "        y1 = int(detect[i, 4] * h)\n",
    "        x2 = int(detect[i, 5] * w)\n",
    "        y2 = int(detect[i, 6] * h)\n",
    "\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0))\n",
    "\n",
    "        label = f'Face: {confidence:4.2f}'\n",
    "        cv2.putText(frame, label, (x1, y1 - 1), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('frame', frame)\n",
    "\n",
    "    if cv2.waitKey(1) == 27:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLOv3를 이용한 객체 검출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pjreddie.com/darknet/yolo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "# 모델 & 설정 파일\n",
    "model = 'yolo_v3/yolov3.weights'\n",
    "config = 'yolo_v3/yolov3.cfg'\n",
    "class_labels = 'yolo_v3/coco.names'\n",
    "confThreshold = 0.5\n",
    "nmsThreshold = 0.4\n",
    "\n",
    "# 테스트 이미지 파일\n",
    "img_files = ['yolo_v3/dog.jpg', 'yolo_v3/person.jpg', \n",
    "             'yolo_v3/sheep.jpg', 'yolo_v3/kite.jpg']\n",
    "\n",
    "# 네트워크 생성\n",
    "net = cv2.dnn.readNet(model, config)\n",
    "\n",
    "if net.empty():\n",
    "    print('Net open failed!')\n",
    "    sys.exit()\n",
    "\n",
    "# 클래스 이름 불러오기\n",
    "\n",
    "classes = []\n",
    "with open(class_labels, 'rt') as f:\n",
    "    classes = f.read().rstrip('\\n').split('\\n')\n",
    "\n",
    "colors = np.random.uniform(0, 255, size=(len(classes), 3))\n",
    "\n",
    "# 출력 레이어 이름 받아오기\n",
    "\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "# output_layers = ['yolo_82', 'yolo_94', 'yolo_106']\n",
    "\n",
    "# 실행\n",
    "\n",
    "for f in img_files:\n",
    "    img = cv2.imread(f)\n",
    "\n",
    "    if img is None:\n",
    "        continue\n",
    "\n",
    "    # 블롭 생성 & 추론\n",
    "    blob = cv2.dnn.blobFromImage(img, 1/255., (320, 320), swapRB=True)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers) #\n",
    "\n",
    "    # outs는 3개의 ndarray 리스트.\n",
    "    # outs[0].shape=(507, 85), 13*13*3=507\n",
    "    # outs[1].shape=(2028, 85), 26*26*3=2028\n",
    "    # outs[2].shape=(8112, 85), 52*52*3=8112\n",
    "\n",
    "    h, w = img.shape[:2]\n",
    "\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            # detection: 4(bounding box) + 1(objectness_score) + 80(class confidence)\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > confThreshold:\n",
    "                # 바운딩 박스 중심 좌표 & 박스 크기\n",
    "                cx = int(detection[0] * w)\n",
    "                cy = int(detection[1] * h)\n",
    "                bw = int(detection[2] * w)\n",
    "                bh = int(detection[3] * h)\n",
    "\n",
    "                # 바운딩 박스 좌상단 좌표\n",
    "                sx = int(cx - bw / 2)\n",
    "                sy = int(cy - bh / 2)\n",
    "\n",
    "                boxes.append([sx, sy, bw, bh])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(int(class_id))\n",
    "\n",
    "    # 비최대 억제, Non Max Suppression\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, confThreshold, nmsThreshold)\n",
    "\n",
    "    for i in indices:\n",
    "        i = i[0]\n",
    "        sx, sy, bw, bh = boxes[i]\n",
    "        label = f'{classes[class_ids[i]]}: {confidences[i]:.2}'\n",
    "        color = colors[class_ids[i]]\n",
    "        cv2.rectangle(img, (sx, sy, bw, bh), color, 2)\n",
    "        cv2.putText(img, label, (sx, sy - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2, cv2.LINE_AA)\n",
    "\n",
    "    t, _ = net.getPerfProfile()\n",
    "    label = 'Inference time: %.2f ms' % (t * 1000.0 / cv2.getTickFrequency())\n",
    "    cv2.putText(img, label, (10, 30), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.7, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('img', img)\n",
    "    cv2.waitKey()\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}